[
{
	"uri": "/en/",
	"title": "Optimizing cost with AWS Graviton2 based services",
	"tags": [],
	"description": "",
	"content": "AWS Graviton processors are custom built by Amazon Web Services using 64-bit Arm Neoverse cores to deliver the best price performance for your cloud workloads running in Amazon EC2. Amazon EC2 provides the broadest and deepest portfolio of compute instances, including many that are powered by latest-generation Intel and AMD processors. AWS Graviton processors add even more choice to help customers optimize performance and cost for their workloads.\nAWS Graviton2 processors deliver a major leap in performance and capabilities over first-generation AWS Graviton processors. They power Amazon EC2 T4g, M6g, C6g, and R6g instances, and their variants with local NVMe-based SSD storage, that provide up to 40% better price performance over comparable current generation x86-based instances1 for a wide variety of workloads, including application servers, micro-services, high-performance computing, electronic design automation, gaming, open-source databases, and in-memory caches. The AWS Graviton2 processors also provide enhanced performance for video encoding workloads, hardware acceleration for compression workloads, and support for CPU-based machine learning inference. They deliver 7x more performance, 4x more compute cores, 5x faster memory, and 2x larger caches.\n"
},
{
	"uri": "/en/amazoncontainers/creatingpipeline.html",
	"title": "Create container build pipeline",
	"tags": [],
	"description": "",
	"content": "We will start by creating a pipeline for building multi-architecture container images. Creating images for both x86 and arm64 architectures will allow us to explore native multi-architecture support in AWS services and freely choose between the most optimal compute option for the workload.\nThe CDK stack you will deploy will create a source code repository for the NodeJS application and two build steps to create container images for x86 and arm64 architectures.\n Start by reviewing the contents of the CDK stack definition in file graviton-labs/graviton2/cs_graviton/pipeline_graviton2.py for both architectures.  In order to build multi-architecture images we are not using any emulation layer. Instead we use both x86 and arm64 build instances to ensure we build our containers on native architectures.\n x86 container build :\n docker_build_x86 = codebuild.PipelineProject( scope=self, id=f\u0026quot;DockerBuild_x86\u0026quot;, environment=dict( build_image=codebuild.LinuxBuildImage.AMAZON_LINUX_2_3, privileged=True), environment_variables={ 'REPO_ECR': codebuild.BuildEnvironmentVariable( value=container_repository.repository_uri), }, build_spec=buildspec_x86 ) arm64 container build :\n docker_build_arm64 = codebuild.PipelineProject( scope=self, id=f\u0026quot;DockerBuild_ARM64\u0026quot;, environment=dict( build_image=codebuild.LinuxBuildImage.AMAZON_LINUX_2_ARM, privileged=True), environment_variables={ 'REPO_ECR': codebuild.BuildEnvironmentVariable( value=container_repository.repository_uri), }, build_spec=buildspec_arm64 ) After successful build the continer images will be pushed to dedicated Elastic Container Registy (ECR) with tags following \u0026lt;COMMIT_HASH\u0026gt;: pattern. At this point, you could pull these images by referring to their architecture-specific tags\nWe will simplify this process by creating a docker multi-arch manifest list and pushing it to Amazon ECR.\n The end-to-end container build will match the diagram below.\nInitiate the pipeline deployment by issuing command:  cdk deploy GravitonID-pipeline Once the multi-architecture build pipeline is deployed you can proceed to the next step where we build containers for x86 and arm64 version of NodeJS application.  "
},
{
	"uri": "/en/amazonrds/01_snapshot/creatingcluster8.html",
	"title": "Create RDS MySQL instance",
	"tags": [],
	"description": "",
	"content": "Start by reviewing contents of the RDS MySQL 8 CDK stack definition in file graviton2_labs/rds_graviton/rds_mysql_8.py\n db_mysql8 = rds.DatabaseInstance(self, \u0026quot;MySQL8\u0026quot;, engine=rds.DatabaseInstanceEngine.mysql( version=rds.MysqlEngineVersion.VER_8_0_21 ), instance_type=ec2.InstanceType(\u0026quot;m5.4xlarge\u0026quot;), vpc=vpc, multi_az=False, publicly_accessible=True, allocated_storage=100, storage_type=rds.StorageType.IO1, iops=5000, cloudwatch_logs_exports=[\u0026quot;error\u0026quot;, \u0026quot;general\u0026quot;, \u0026quot;slowquery\u0026quot;], deletion_protection=False, enable_performance_insights=True, delete_automated_backups=True, backup_retention=core.Duration.days(1), parameter_group=rds.ParameterGroup.from_parameter_group_name( self, \u0026quot;para-group-mysql\u0026quot;, parameter_group_name=\u0026quot;default.mysql8.0\u0026quot; ) )  Use of AWS Cloud Development Kit (CDK) greatly simplifies instantiation of AWS services such as Amazon RDS. Above Python code fragment allows you instantiate single-AZ RDS MySQL 8 instance with 100GB IO1 Amazon EBS volume. This instance is using m5.4xlarge instance type and will be accessible onlyfrom your Cloud9 environment via public IP. RDS instance created though CDK will also have simple backup policy, logging, and RDS Performance Insights enabled.\n Initiate RDS MySQL 8 deployment using :  cdk deploy GravitonID-rds-8 Before you connect to the MySQL instance, we need to set the username and password as environment variables in our Cloud9 session. We will use CloudFormation output to find relevant variables.  CREDS=$(aws secretsmanager get-secret-value --secret-id `aws cloudformation describe-stacks --stack-name GravitonID-rds-8 --query \u0026#34;Stacks[0].Outputs[0].OutputValue\u0026#34; --output text` | jq -r \u0026#39;.SecretString\u0026#39;) export DBUSER=\u0026#34;`echo $CREDS| jq -r \u0026#39;.username\u0026#39;`\u0026#34; export DBPASS=\u0026#34;`echo $CREDS| jq -r \u0026#39;.password\u0026#39;`\u0026#34; export DBHOST=\u0026#34;`echo $CREDS| jq -r \u0026#39;.host\u0026#39;`\u0026#34; export DBPORT=\u0026#34;`echo $CREDS| jq -r \u0026#39;.port\u0026#39;`\u0026#34; echo $DBUSER echo $DBPASS echo $DBHOST echo $DBPORT If you see an output of your username, password, host, and database port you have correctly set environment variables. Proceed to the next chapter \u0026ldquo;Import sample database\u0026rdquo;.  "
},
{
	"uri": "/en/amazonrds/02_upgrade/creatingcluster5.html",
	"title": "Create RDS MySQL instance",
	"tags": [],
	"description": "",
	"content": "Start by reviewing contents of the RDS MySQL 5 task definition in file labs/rds_graviton/rds_mysql_5.py\n db_mysql5 = rds.DatabaseInstance(self, \u0026quot;MySQL5\u0026quot;, engine=rds.DatabaseInstanceEngine.mysql( version=rds.MysqlEngineVersion.VER_5_7_31 ), instance_type=ec2.InstanceType(\u0026quot;m5.4xlarge\u0026quot;), vpc=vpc, multi_az=False, publicly_accessible=True, allocated_storage=100, storage_type=rds.StorageType.GP2, cloudwatch_logs_exports=[\u0026quot;audit\u0026quot;, \u0026quot;error\u0026quot;, \u0026quot;general\u0026quot;, \u0026quot;slowquery\u0026quot;], deletion_protection=False, enable_performance_insights=True, delete_automated_backups=True, backup_retention=core.Duration.days(1), parameter_group=rds.ParameterGroup.from_parameter_group_name( self, \u0026quot;para-group-mysql\u0026quot;, parameter_group_name=\u0026quot;default.mysql5.7\u0026quot; ) )  Use of AWS Cloud Development Kit (CDK) greatly simplifies instantiation of AWS services such as Amazon RDS. Above Python code fragment allows you instantiate single-AZ RDS MySQL 5 instance with 100GB GP2 Amazon EBS volume. This instance is using m5.4xlarge instance type and will be accessible onlyfrom your Cloud9 environment via public IP. RDS instance created though CDK will also have simple backup policy, logging, and RDS Performance Insights enabled.\n Initiate RDS MySQL 5 deployment using :  cdk deploy GravitonID-rds-5 Before we connect to the MySQL instance, we need to set the username and password as environment variables in our Cloud9 session. We will use CloudFormation output to find relevant variables.  CREDS=$(aws secretsmanager get-secret-value --secret-id `aws cloudformation describe-stacks --stack-name GravitonID-rds-5 --query \u0026#34;Stacks[0].Outputs[0].OutputValue\u0026#34; --output text` | jq -r \u0026#39;.SecretString\u0026#39;) export DBID=$(aws cloudformation describe-stacks --stack-name GravitonID-rds-5 --query \u0026#34;Stacks[0].Outputs[1].OutputValue\u0026#34; --output text) export DBUSER=\u0026#34;`echo $CREDS| jq -r \u0026#39;.username\u0026#39;`\u0026#34; export DBPASS=\u0026#34;`echo $CREDS| jq -r \u0026#39;.password\u0026#39;`\u0026#34; export DBHOST=\u0026#34;`echo $CREDS| jq -r \u0026#39;.host\u0026#39;`\u0026#34; export DBPORT=\u0026#34;`echo $CREDS| jq -r \u0026#39;.port\u0026#39;`\u0026#34; echo $DBUSER echo $DBPASS echo $DBHOST echo $DBPORT echo $DBID If you see an output of your username, password, host, and database port you have correctly set environment variables. Proceed to the next chapter \u0026ldquo;Import sample database\u0026rdquo;.  "
},
{
	"uri": "/en/gettingstarted.html",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": "This first step is focused on deploying all prerequisites for Graviton2 Labs. For the purpose of this lab we will use following AWS services and software components:\n Cloud9 IDE AWS CDK (Cloud Development Kit)  We will use the AWS CDK to deploy some prerequisites.\nOur guiding principle is that we\u0026rsquo;ll use the CDK to deploy static infrastructure and prerequisites.\nSet up a Cloud9 IDE  In the AWS console, go to the Cloud9 service and select Create environment. Call your new IDE Graviton2IDE and click Next Step.\nOn the next screen, select \u0026ldquo;Other instance type\u0026rdquo; and choose m5.xlarge instance type. Click Next step again.\nOn the final page, click Create environment. Make sure that you leave the VPC settings at the default values.  Once the environment builds, you\u0026rsquo;ll automatically redirect to the IDE. Take a minute to explore the interface, and note that you can change the color scheme if you like (AWS Cloud9 menu -\u0026gt; Preferences -\u0026gt; Themes).\nNext, let\u0026rsquo;s update the Cloud9 environment to let you run the labs from the environment.\n  Create a role for your Cloud9 environment by clicking on the following link\n  Confirm that AWS service and EC2 are selected, then click Next to view permissions.\n  Confirm that AdministratorAccess is checked, then click Next: Tags to assign tags.\n  Leave the defaults, and click Next: Review to review.\n  Enter Cloud9-Admin-Role for the Name, and click Create role.\n  Once this new profile is created, go to EC2 and find the Cloud9 instance, and assign the instance profile to this instance.\n  Go to Cloud9 Preferences and under AWS Credentials disable AWS managed temporary credentials.\n  Move to the \u0026ldquo;Prerequisites\u0026rdquo; step  "
},
{
	"uri": "/en/amazonemr/creatingcluster.html",
	"title": "Creating EMR Claster with Graviton2 instanaces",
	"tags": [],
	"description": "",
	"content": " Create S3 bucket for EMR jobs.  cd ~/environment/graviton2-labs source scripts/create_emr_buckets.sh Execute following command to create EMR cluster in your default VPC.  export EMR_CLUSTER_ID=$(aws emr create-cluster --name test-emr-cluster --use-default-roles --release-label emr-6.1.0 --instance-count 3 --instance-type m6g.xlarge --applications Name=JupyterHub Name=Spark --ec2-attributes KeyName=graviton2key | jq -r \u0026#39;.ClusterId\u0026#39;); echo \u0026#34;Your cluster ID is = $EMR_CLUSTER_ID\u0026#34; Authorize SSH connection from Cloud9 instance.  export C9_PRIVATEIP=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r \u0026#39;.privateIp\u0026#39;) export EMR_MASTER_SG=$(aws emr describe-cluster --cluster-id $EMR_CLUSTER_ID | jq -r \u0026#39;.Cluster | .Ec2InstanceAttributes | .EmrManagedMasterSecurityGroup\u0026#39;) export EMR_MASTER_IP=$(aws emr describe-cluster --cluster-id $EMR_CLUSTER_ID | jq -r \u0026#39;.Cluster | .MasterPublicDnsName\u0026#39;) aws ec2 authorize-security-group-ingress --group-id $EMR_MASTER_SG --protocol tcp --port 22 --cidr $C9_PRIVATEIP/32 Copy Spark ETL job to EMR Master node  scp scripts/etl-spark.py hadoop@$EMR_MASTER_IP:~/ Connect to EMR Master node and execute sparc job  ssh hadoop@$EMR_MASTER_IP \u0026#34;spark-submit etl-spark.py s3://\u0026#39;$emr_s3_name\u0026#39;/input/ s3://\u0026#39;$emr_s3_name\u0026#39;/output/spark\u0026#34; When Spark job submitted through spark-submit on the command line, it shows up logs on the console.\nWait few minutes and navigate to S3 service console to check “output/spark” folder in your S3 bucket to see the results.  "
},
{
	"uri": "/en/amazonrds/01_snapshot/loadingdata.html",
	"title": "Import Sample Database",
	"tags": [],
	"description": "",
	"content": " Before we start this lab, please make sure you are working in your graviton2-labs folder. If you are not, please change to the folder by executing the following command in your bash shell :  cd ~/environment/graviton2-labs/ We\u0026rsquo;ll start by cloning a sample database for MySQL.\ngit clone https://github.com/datacharmer/test_db.git cd test_db After you have done this, please connect to your MySQL instance. Run the following command to connect to your MySQL instance:  mysql -h $DBHOST -P $DBPORT -u$DBUSER -p$DBPASS If you are experiencing log in issues due to the lack of environment variables, please use the following commands to set the credentials.\nCREDS=$(aws secretsmanager get-secret-value --secret-id `aws cloudformation describe-stacks --stack-name GravitonID-rds-8 --query \u0026#34;Stacks[0].Outputs[0].OutputValue\u0026#34; --output text` | jq -r \u0026#39;.SecretString\u0026#39;) export DBUSER=\u0026#34;`echo $CREDS| jq -r \u0026#39;.username\u0026#39;`\u0026#34; export DBPASS=\u0026#34;`echo $CREDS| jq -r \u0026#39;.password\u0026#39;`\u0026#34; export DBHOST=\u0026#34;`echo $CREDS| jq -r \u0026#39;.host\u0026#39;`\u0026#34; export DBPORT=\u0026#34;`echo $CREDS| jq -r \u0026#39;.port\u0026#39;`\u0026#34; echo $DBUSER echo $DBPASS echo $DBHOST echo $DBPORT If you are logged in to your MySQL instance you can load a sample database to your RDS instance. To load the date into the database, run the following command from your MySQL client.  source employees.sql The engine should start loading the data on its own and you should see outputs similar to:\nQuery OK, 7671 rows affected (0.05 sec) Records: 7671 Duplicates: 0 Warnings: 0 +---------------------+ | data_load_time_diff | +---------------------+ | 00:00:33 | +---------------------+ 1 row in set (0.00 sec) mysql\u0026gt; Verify the employees table checksum by running :  use employees; checksum table employees; You should see the output similar to one below. Please write down the checksum value for original database table checksum.\n+---------------------+-----------+ | Table | Checksum | +---------------------+-----------+ | employees.employees | 610052939 | +---------------------+-----------+ 1 row in set (0.24 sec) Exit MySQL client and create database snapshot by executing following command:  cd ~/environment/graviton2-labs/ ~/environment/graviton2-labs/scripts/rds-snapshot.sh The script will return a randomly generated snapshot id.\nPlease monitor the snapshot progress using AWS console or by executing following command :  aws rds describe-db-snapshots --db-snapshot-identifier `aws ssm get-parameter --name \u0026#34;graviton_rds_lab_snapshot\u0026#34; | jq -r .Parameter.Value` | jq -r .DBSnapshots[0].Status When snapshot status changes to \u0026ldquo;available\u0026rdquo; you can procedd to the next chapter.  "
},
{
	"uri": "/en/amazonrds/02_upgrade/loadingdata.html",
	"title": "Import Sample Database",
	"tags": [],
	"description": "",
	"content": " Before we start this lab, please make sure you are working in your graviton2-labs folder. If you are not, please change to the folder by executing the following command in your bash shell :  cd ~/environment/graviton2-labs/ We\u0026rsquo;ll start by cloning a sample database for MySQL.\ngit clone https://github.com/datacharmer/test_db.git cd test_db After you have done this, please connect to your MySQL instance. Run the following command to connect to your MySQL instance:  mysql -h $DBHOST -P $DBPORT -u$DBUSER -p$DBPASS If you are experiencing log in issues due to the lack of environment variables, please use the following commands to set the credentials.\nCREDS=$(aws secretsmanager get-secret-value --secret-id `aws cloudformation describe-stacks --stack-name GravitonID-rds-5 --query \u0026#34;Stacks[0].Outputs[0].OutputValue\u0026#34; --output text` | jq -r \u0026#39;.SecretString\u0026#39;) export DBUSER=\u0026#34;`echo $CREDS| jq -r \u0026#39;.username\u0026#39;`\u0026#34; export DBPASS=\u0026#34;`echo $CREDS| jq -r \u0026#39;.password\u0026#39;`\u0026#34; export DBHOST=\u0026#34;`echo $CREDS| jq -r \u0026#39;.host\u0026#39;`\u0026#34; export DBPORT=\u0026#34;`echo $CREDS| jq -r \u0026#39;.port\u0026#39;`\u0026#34; echo $DBUSER echo $DBPASS echo $DBHOST echo $DBPORT If you are logged in to your MySQL instance you can load a sample database to your RDS instance. To load the date into the database, run the following command from your MySQL client.  source employees.sql The engine should start loading the data on its own and you should see outputs similar to:\nQuery OK, 7671 rows affected (0.05 sec) Records: 7671 Duplicates: 0 Warnings: 0 +---------------------+ | data_load_time_diff | +---------------------+ | 00:00:33 | +---------------------+ 1 row in set (0.00 sec) mysql\u0026gt; Verify the employees table checksum by running :  use employees; checksum table employees; You should see the output similar to one below. Please write down the checksum value for original database table checksum.\n+---------------------+-----------+ | Table | Checksum | +---------------------+-----------+ | employees.employees | 610052939 | +---------------------+-----------+ 1 row in set (0.24 sec) Exit MySQL client  Please write down the checksum value for original MySQL 5 database table checksum. Exit the database client and moveto the next step \u0026ldquo;Upgrading RDS MySQL database\u0026rdquo;.\n"
},
{
	"uri": "/en/amazoncontainers/multiarchitecture.html",
	"title": "Multi-architecture application build",
	"tags": [],
	"description": "",
	"content": " Before we start this lab, please make sure you are working in your graviton2-labs folder. If you are not, please change to the folder by executing the following command in your bash shell :  cd ~/environment/graviton2-labs/ Clone an empty CodeCommit repo by executing :  git clone `aws cloudformation describe-stacks --stack-name GravitonID-pipeline --query \u0026#34;Stacks[0].Outputs[0].OutputValue\u0026#34; --output text` Copy code build specification files and NodeJS application source code to the new git repo.  cp -r graviton2/cs_graviton/app/* graviton2-pipeline-lab/ Execute below commands to commit and push changes to CodeCommit repository.  cd graviton2-pipeline-lab git add . git commit -m \u0026#34;First commit Node.js sample application.\u0026#34; git push This will trigger\n build of docker images for both x86 and arm64 architectures build of docker image with multi-architecture manifest a push of all docker images to ECS repository   Monitor CodePipeline steps using AWS Console by opening this link in a new tab. https://console.aws.amazon.com/codesuite/codepipeline/pipelines/graviton2-pipeline-lab/view?region=us-east-1\n  When all steps of CodePipeline are successful, execute the following commands to set environment variables:\n  export ECR_REPO_URI=$(aws ecr describe-repositories --repository-name graviton2-pipeline-lab | jq -r \u0026#39;.repositories[0].repositoryUri\u0026#39;) export MULTI_IMAGE_TAG=$(aws ecr describe-images --repository-name graviton2-pipeline-lab --query \u0026#39;sort_by(imageDetails,\u0026amp; imagePushedAt)[-1].imageTags[0]\u0026#39; | jq -r .) export CONTAINER_URI=$ECR_REPO_URI:$MULTI_IMAGE_TAG echo $ECR_REPO_URI echo $MULTI_IMAGE_TAG echo $CONTAINER_URI aws ssm put-parameter --name \u0026#34;graviton_lab_container_uri\u0026#34; --value $CONTAINER_URI --type String --overwrite "
},
{
	"uri": "/en/gettingstarted/prereqisites.html",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": " In your Cloud 9 environment, clone the Git repo:  git clone https://git-codecommit.us-east-1.amazonaws.com/v1/repos/graviton2-labs cd graviton2-labs Upgrade AWS CLI according to guidance in AWS documentation.  sudo pip install --upgrade awscli \u0026amp;\u0026amp; hash -r Install jq, envsubst (from GNU gettext utilities) and bash-completion  sudo yum -y install jq gettext bash-completion moreutils Install mysql client and libraries  sudo yum -y localinstall https://dev.mysql.com/get/mysql80-community-release-el7-3.noarch.rpm sudo yum -y install mysql-community-client mysql-community-libs Install the required Kubernetes tools for interacting with the EKS cluster.  sudo curl --silent --location -o /usr/local/bin/kubectl \\  https://amazon-eks.s3.us-west-2.amazonaws.com/1.17.11/2020-09-18/bin/linux/amd64/kubectl sudo chmod +x /usr/local/bin/kubectl kubectl completion bash \u0026gt;\u0026gt; ~/.bash_completion . /etc/profile.d/bash_completion.sh . ~/.bash_completion Set account and region environmental variables  export ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) export AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r \u0026#39;.region\u0026#39;) echo \u0026#34;export ACCOUNT_ID=${ACCOUNT_ID}\u0026#34; | tee -a ~/.bash_profile echo \u0026#34;export AWS_REGION=${AWS_REGION}\u0026#34; | tee -a ~/.bash_profile aws configure set default.region ${AWS_REGION} aws configure set default.account ${ACCOUNT_ID} aws configure get default.region aws configure get default.account SSH key  Run this command to generate SSH Key in Cloud9. This key will be used on the worker node instances to allow ssh access if necessary.\nssh-keygen  Press enter 3 times to take the default choices\n Upload the public key to your EC2 region:\naws ec2 import-key-pair --key-name \u0026#34;graviton2key\u0026#34; --public-key-material file://~/.ssh/id_rsa.pub If you got an error similar to An error occurred (InvalidKey.Format) when calling the ImportKeyPair operation: Key is not in valid OpenSSH public key format then you can try this command instead:\naws ec2 import-key-pair --key-name \u0026#34;graviton2key\u0026#34; --public-key-material fileb://~/.ssh/id_rsa.pub Prepare AWS CDK  python3 -m venv .venv source .venv/bin/activate pip install --upgrade aws-cdk.core pip install -r requirements.txt cdk bootstrap aws://$ACCOUNT_ID/$AWS_REGION cdk synth cdk ls "
},
{
	"uri": "/en/amazoncontainers/amazoneks/ekscluster.html",
	"title": "AmazonEKS : Deploy multi-architecture EKS cluster",
	"tags": [],
	"description": "",
	"content": "Amazon Elastic Kubernetes Service (Amazon EKS) is a managed service that you can use to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane or nodes. Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications.\n Start by reviewing the contents of the CDK stack definition in file graviton-labs/graviton2/cs_graviton/eks_graviton2.py. This stack will create an EKS cluster with two nodegroups: x86-node-group and arm64-node-group.   self.cluster = eks.Cluster(self, \u0026quot;EKSGraviton2\u0026quot;, version=eks.KubernetesVersion.V1_18, default_capacity=0, endpoint_access=eks.EndpointAccess.PUBLIC_AND_PRIVATE, vpc=vpc, security_group=eks_security_group ) self.ng_x86 = self.cluster.add_nodegroup_capacity(\u0026quot;x86-node-group\u0026quot;, instance_types=[ec2.InstanceType(\u0026quot;m5.large\u0026quot;)], desired_size=2, min_size=1, max_size=3 ) self.ng_arm64 = self.cluster.add_nodegroup_capacity(\u0026quot;arm64-node-group\u0026quot;, instance_types=[ec2.InstanceType(\u0026quot;m6g.large\u0026quot;)], desired_size=2, min_size=1, max_size=3 ) Deploy EKS cluster with two nodegroups using CDK  cdk deploy GravitonID-eks  This process can take 15-20 minutes to complete. It\u0026rsquo;s a great opportunity to take a short break.\n Once the cluster and the nodegroups are available execute following commands to create kubectl configuration file.  `aws cloudformation describe-stacks --stack-name GravitonID-eks --query \u0026#34;Stacks[0].Outputs[1].OutputValue\u0026#34; --output text` Test the cluster and verify your nodes.  kubectl get nodes --label-columns=kubernetes.io/arch You should see output similar to the one below with four nodes in total (two arm64 nodes using Graviton2 instances, and two amd64 running on x86-64 architecture)\nNAME STATUS ROLES AGE VERSION ARCH ip-10-0-2-105.ec2.internal Ready \u0026lt;none\u0026gt; 10h v1.18.9-eks-d1db3c amd64 ip-10-0-2-193.ec2.internal Ready \u0026lt;none\u0026gt; 10h v1.18.9-eks-d1db3c arm64 ip-10-0-3-28.ec2.internal Ready \u0026lt;none\u0026gt; 10h v1.18.9-eks-d1db3c arm64 ip-10-0-3-58.ec2.internal Ready \u0026lt;none\u0026gt; 10h v1.18.9-eks-d1db3c amd64 You now have a fully working multi-architecture Amazon EKS Cluster that is ready to use.\nContinue to the next step  "
},
{
	"uri": "/en/amazonrds/01_snapshot/restoringrds.html",
	"title": "Restore your snapshot to Graviton2 instance",
	"tags": [],
	"description": "",
	"content": "We\u0026rsquo;re ready to restore our database to new Graviton2-based instance. Start by reviewing contents of the RDS MySQL 8 database definition in file graviton2_labs/rds_graviton/ds_restore.py\n snapshot_id = ssm.StringParameter.value_for_string_parameter(self ,\u0026quot;graviton_rds_lab_snapshot\u0026quot;) g2_db_mysql8 = rds.DatabaseInstanceFromSnapshot(self, \u0026quot;GravitonMySQL\u0026quot;, engine=rds.DatabaseInstanceEngine.mysql( version=rds.MysqlEngineVersion.VER_8_0_21 ), instance_type=ec2.InstanceType(\u0026quot;m6g.4xlarge\u0026quot;), snapshot_identifier=snapshot_id, vpc=vpc, multi_az=False, publicly_accessible=True, allocated_storage=100, storage_type=rds.StorageType.IO1, iops=5000, cloudwatch_logs_exports=[\u0026quot;error\u0026quot;, \u0026quot;general\u0026quot;, \u0026quot;slowquery\u0026quot;], enable_performance_insights=True, deletion_protection=False, delete_automated_backups=True, backup_retention=core.Duration.days(1), vpc_subnets={ \u0026quot;subnet_type\u0026quot;: ec2.SubnetType.PUBLIC }, parameter_group=rds.ParameterGroup.from_parameter_group_name( self, \u0026quot;para-group-mysql\u0026quot;, parameter_group_name=\u0026quot;default.mysql8.0\u0026quot; ) )  We\u0026rsquo;ll use the snapshot we created from original x86 instance to restore new Graviton2 database. Execute following command to create a new Graviton2 instance:  cdk deploy GravitonID-rds-restore Before we connect to the Graviton2 MySQL instance, we need to update the username and password as environment variables in our Cloud9 session. We will use CloudFormation output to find relevant variables.  CREDS=$(aws secretsmanager get-secret-value --secret-id `aws cloudformation describe-stacks --stack-name GravitonID-rds-8 --query \u0026#34;Stacks[0].Outputs[0].OutputValue\u0026#34; --output text` | jq -r \u0026#39;.SecretString\u0026#39;) export DBUSER=\u0026#34;`echo $CREDS| jq -r \u0026#39;.username\u0026#39;`\u0026#34; export DBPASS=\u0026#34;`echo $CREDS| jq -r \u0026#39;.password\u0026#39;`\u0026#34; export DBPORT=\u0026#34;`echo $CREDS| jq -r \u0026#39;.port\u0026#39;`\u0026#34; export DBHOST=$(aws rds describe-db-instances --db-instance-identifier `aws cloudformation describe-stacks --stack-name GravitonID-rds-restore --query \u0026#34;Stacks[0].Outputs[0].OutputValue\u0026#34; --output text` | jq -r \u0026#39;.DBInstances[] | .Endpoint.Address\u0026#39;) echo $DBUSER echo $DBPASS echo $DBHOST echo $DBPORT After you have done this, please connect to your MySQL instance. Run the following command to connect to your MySQL instance:  mysql -h $DBHOST -P $DBPORT -u$DBUSER -p$DBPASS Verify the employees table checksum on Graviton2 instance by running below command in mysql client:  use employees; checksum table employees; You should see the output similar to one below. Please compare checksum value with one from original database.\n+---------------------+-----------+ | Table | Checksum | +---------------------+-----------+ | employees.employees | 610052939 | +---------------------+-----------+ 1 row in set (0.24 sec) Exit MySQL client  At this point you have a fully functional copy of your original database using Amazon RDS and Graviton2 M6g instace type. "
},
{
	"uri": "/en/amazonrds/02_upgrade/upgradingmysq.html",
	"title": "Upgrading RDS MySQL database version",
	"tags": [],
	"description": "",
	"content": "When Amazon RDS starts supporting a new version of a database engine, you can upgrade your DB instances to the new version. There are two kinds of upgrades for MySQL DB instances: major version upgrades and minor version upgrades.\nMajor version upgrades can contain database changes that are not backward-compatible with existing applications. As a result, you must manually perform major version upgrades of your DB instances. You can initiate a major version upgrade by modifying your DB instance.\nIn this exercise you will walk through the steps of upgrading your MySQL engine for your RDS instance. We will perform these operations from Cloud9 using AWS CLI, however you can achive the same results using AWS Console, or modify-db-instance API call.\nWe recommend that you test any changes on a test instance before modifying a production instance, so that you fully understand the impact of each change.\n  Start major version upgrade process by executing following command in your Cloud9 environment :  aws rds modify-db-instance --db-instance-identifier `aws cloudformation describe-stacks --stack-name GravitonID-rds-5 --query \u0026quot;Stacks[0].Outputs[1].OutputValue\u0026quot; --output text` --engine-version 8.0.21 --allow-major-version-upgrade --apply-immediately You can monitor database upgrade process using AWS Console or CLI below  aws rds describe-db-instances --db-instance-identifier $DBID | jq -r .DBInstances[0].DBInstanceStatus Once the upgrade process finished and your database is in \u0026ldquo;availabe\u0026rdquo; connect to your upgraded MySQL instance by running the following command:  mysql -h $DBHOST -P $DBPORT -u$DBUSER -p$DBPASS Verify the employees table checksum on Graviton2 instance by running :  use employees; checksum table employees; You should see the output similar to one below. Please compare checksum value with one from original database.  +---------------------+-----------+ | Table | Checksum | +---------------------+-----------+ | employees.employees | 610052939 | +---------------------+-----------+ 1 row in set (0.24 sec) Exit MySQL client  "
},
{
	"uri": "/en/amazoncontainers/amazoneks/k8sapplication.html",
	"title": "AmazonEKS : Deploy a multi-architecture application",
	"tags": [],
	"description": "",
	"content": "Now you\u0026rsquo;re ready to deploy a multi-architecture Kubernetes Application.\n Start by reviewing contents of the Kubernetes deployment manifest in file graviton2-labs/graviton2/cs_graviton/Deployment.yaml  apiVersion: apps/v1 kind: Deployment metadata: name: multiarch-deployment namespace: multiarch labels: app: multiarch-app spec: replicas: 4 selector: matchLabels: app: multiarch-app template: metadata: labels: app: multiarch-app spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: beta.kubernetes.io/arch operator: In values: - amd64 - arm64 podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - multiarch-app topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; containers: - name: multiarch-container image: {{container_uri}} ports: - containerPort: 80 This manifest will deploy four pods across all EKS nodes (both amd64 and arm64).\nUse the following command to initiate the deployment :  cd ~/environment/graviton2-labs CONTAINER_URI=`aws ssm get-parameter --name \u0026#34;graviton_lab_container_uri\u0026#34; | jq -r .Parameter.Value` cat graviton2/cs_graviton/Deployment.yaml | sed \u0026#34;s#{{container_uri}}#$CONTAINER_URI#\u0026#34; | kubectl apply -f - You can ckeck the status of your deployment by executing:  kubectl get svc,deployment -n multiarch Access multi-architecture application via LoadBalancer endpoint  It will take several minutes for the ELB to become healthy and start passing traffic to the pods.\n ELB=$(kubectl get service multiarch-service -n multiarch -o json | jq -r \u0026#39;.status.loadBalancer.ingress[].hostname\u0026#39;) for i in {1..8}; do curl -m3 $ELB ; echo; done  You can also copy/paste the loadBalancer hostname into your browser and see the application running.\n Please note we are using a single image tag with Amazon ECR, and Amazon EKS. Native multi-architecture support to automatically allows to select the right container for each of nodegroups.\n Congratulations! Now you have deployed multi-architecture application across two nodegroups using x86 and ARM64 instance types. Optional: You can verify a Failure scenario by forcing the use of a single architecture docker image. Start by scaling down the deployment, changing the image tag to point to architecture-specific build, and scaling the deployment back up to the original number of replicas.\nkubectl -n multiarch scale deployment multiarch-deployment --replicas=0 kubectl -n multiarch set image deployment/multiarch-deployment multiarch-container=$CONTAINER_URI-x86 kubectl -n multiarch scale deployment multiarch-deployment --replicas=4 After deployment stabilizes, execute the follwoing command to check deployment status:\nkubectl get pod -n multiarch You should be able to see output similar to the one below where only two pods report \u0026ldquo;Running\u0026rdquo;, while two show \u0026ldquo;CrashLoopBackOff\u0026rdquo; or \u0026ldquo;Error\u0026rdquo; status.\nNAME READY STATUS RESTARTS AGE multiarch-deployment-857dbcdd7c-5455t 0/1 CrashLoopBackOff 6 9m33s multiarch-deployment-857dbcdd7c-l48zd 1/1 Running 0 9m33s multiarch-deployment-857dbcdd7c-ptf65 1/1 Running 0 9m33s multiarch-deployment-857dbcdd7c-xk8vh 0/1 CrashLoopBackOff 6 9m33s "
},
{
	"uri": "/en/amazonrds/02_upgrade/modyfyinginstancetype.html",
	"title": "Changing instance to Graviton2",
	"tags": [],
	"description": "",
	"content": "You can change the settings of a DB instance to accomplish tasks such as adding additional storage or changing the DB instance class. This includes switchin from x86 instances to Graviton2 instance type.\nWe recommend that you test any changes on a test instance before modifying a production instance, so that you fully understand the impact of each change. As mentioned in previous chapter, testing is especially important when upgrading database versions.\nMost modifications to a DB instance you can either apply immediately or defer until the next maintenance window.\n Start instance type change process by executing following command in your Cloud9 environment :   aws rds modify-db-instance --db-instance-identifier `aws cloudformation describe-stacks --stack-name GravitonID-rds-5 --query \u0026quot;Stacks[0].Outputs[1].OutputValue\u0026quot; --output text` --db-instance-class db.m6g.4xlarge --allow-major-version-upgrade --apply-immediately You can monitor database upgrade process using AWS Console or CLI below  aws rds describe-db-instances --db-instance-identifier $DBID | jq -r .DBInstances[0].DBInstanceStatus Once the upgrade process finished and your database is in \u0026ldquo;Availabe\u0026rdquo; connect to your upgraded MySQL instance by running the following command:  mysql -h $DBHOST -P $DBPORT -u$DBUSER -p$DBPASS Verify the employees table checksum on Graviton2 instance by running :  use employees; checksum table employees; You should see the output similar to one below. Please compare checksum value with one from original database.  +---------------------+-----------+ | Table | Checksum | +---------------------+-----------+ | employees.employees | 610052939 | +---------------------+-----------+ 1 row in set (0.24 sec) Exit MySQL client  Please refer to https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.DBInstance.Modifying.html for more details on modifying RDS database instances.\n"
},
{
	"uri": "/en/amazonrds/01_snapshot.html",
	"title": "Option 1: Restoring Graviton2 MySQL RDS database from x86 snapshot",
	"tags": [],
	"description": "",
	"content": "In this Chapter, we will restore a Graviton2 MySQL RDS instance using a snapshot taken from a x86 MySQL RDS instance.\n"
},
{
	"uri": "/en/amazonemr/cleanup.html",
	"title": "EMR Cleanup",
	"tags": [],
	"description": "",
	"content": "To cleanup the EMR cluster please execute following command :\naws emr terminate-clusters --cluster-ids $EMR_CLUSTER_ID After reviewing the contents of S3 bucket you can elect to empty and delete job outputs via AWS S3 console or using CLI.\n"
},
{
	"uri": "/en/amazoncontainers/amazoneks/aspnet.html",
	"title": "AmazonEKS : Running ASP.NET Core application on Graviton2",
	"tags": [],
	"description": "",
	"content": "One of the great strengths of the .NET Core platform .NET 5 is signifcant improvement in runtime performance and that .NET is ready for scalable modernization efforts. Customers can also choose to build their .NET Core platform with AWS purpose-built AWS Graviton2 Processor. AWS is the only major cloud provider that enables .NET 5 an ARM64 architecture option today. Amazon EC2 instances running AWS Graviton2 provide up to 40% better price performance over comparable current generation x86-based instances. AWS Graviton2 helps customers running .NET realize ARM64 performance improvements with all .NET 5 Linux supported distributions (Alpine Linux, Debian, and Ubuntu).\nMake sure you complete AmazonEKS : \u0026ldquo;Deploy multi-architecture EKS cluster\u0026rdquo; lab before starting this one.\n Start by reviewing CDK stack responsible for deploying .NET Core build pipeline in graviton2-labs/graviton2/cs_graviton/pipeline_netcore_graviton2.py file.\n docker_build_arm64 = codebuild.PipelineProject( scope=self, id=f\u0026quot;DockerBuild_ARM64\u0026quot;, environment=dict( build_image=codebuild.LinuxBuildImage.AMAZON_LINUX_2_ARM, privileged=True), environment_variables={ 'REPO_ECR': codebuild.BuildEnvironmentVariable( value=container_repository.repository_uri), }, build_spec=buildspec_arm64 )   Deploy a .NET Core build pipeline using CDK  cd ~/environment/graviton2-labs/ cdk deploy GravitonID-pipeline-dotnet Clone .NET Core samples git repository by executing :  git clone https://github.com/dotnet/dotnet-docker.git Clone .NET Core Code Commit git repository by executing :  git clone `aws cloudformation describe-stacks --stack-name GravitonID-pipeline-dotnet --query \u0026#34;Stacks[0].Outputs[0].OutputValue\u0026#34; --output text` Copy code build specification files and .Net Core 5 application source code to the new git repo.  cp -r dotnet-docker/samples/aspnetapp/* graviton2-aspnet-lab/ cp dotnet-docker/samples/aspnetapp/Dockerfile.debian-arm64 graviton2-aspnet-lab/Dockerfile cp graviton2/cs_graviton/arm64-dotnet-buildspec.yml graviton2-aspnet-lab/ Execute below commands to commit and push changes to CodeCommit repository.  cd ~/environment/graviton2-labs/graviton2-aspnet-lab/ git add . git commit -m \u0026#34;.Net Core 5 ASP Net Sample First Commit\u0026#34; git push This will trigger\n build of docker image of .Net Core 5 application for arm64 architectures Push of .Net Core 5 application docker images to ECS repository   Monitor CodePipeline steps using AWS Console by opening this link in a new tab. Remember to choose the right AWS region. https://console.aws.amazon.com/codesuite/codepipeline/pipelines/\n  When the all steps of CodePipeline are successful return to Cloud9 environment and execute the following command to set environment variables for deployemnt.\n  export NET_REPO_URI=$(aws ecr describe-repositories --repository-name graviton2-aspnet-lab | jq -r \u0026#39;.repositories[0].repositoryUri\u0026#39;) export NET_IMAGE_TAG=$(aws ecr describe-images --repository-name graviton2-aspnet-lab --query \u0026#39;sort_by(imageDetails,\u0026amp; imagePushedAt)[-1].imageTags[0]\u0026#39; | jq -r .) export NET_CONTAINER_URI=$NET_REPO_URI:$NET_IMAGE_TAG echo $NET_REPO_URI echo $NET_IMAGE_TAG echo $NET_CONTAINER_URI aws ssm put-parameter --name \u0026#34;graviton_net_container_uri\u0026#34; --value $NET_CONTAINER_URI --type String --overwrite Deploy .Net Core application to EKS cluster  cd ~/environment/graviton2-labs/ cat graviton2/cs_graviton/Deployment-aspnet.yaml | sed \u0026#34;s#{{container_uri}}#$NET_CONTAINER_URI#\u0026#34; | kubectl apply -f - You can ckeck the status of your deployment by executing:  kubectl get svc,deployment,po -n aspnet Access .Net Core application via LoadBalancer endpoint  It will take several minutes for the ELB to become healthy and start passing traffic to the pods.\n NET_ELB=$(kubectl get service aspnet-service -n aspnet -o json | jq -r \u0026#39;.status.loadBalancer.ingress[].hostname\u0026#39;) echo $NET_ELB Copy and paste the loadBalancer dns name into your browser and see the application running.\nCongratulations! Now you have deployed .Net Core 5 application using Elastic Kubernetes Service and ARM64 instance types. "
},
{
	"uri": "/en/amazonrds/02_upgrade.html",
	"title": "Option 2 : MySQL RDS upgrade",
	"tags": [],
	"description": "",
	"content": "In this Chapter, we will start by upgrading MySQL 5 RDS instance to new version of MySQL first and then modyfying instance type to Graviton2. Such modifications are usually scheduled to happen during your next maintenance window, and a short service interruption is expected to occur.\n"
},
{
	"uri": "/en/amazoncontainers.html",
	"title": "Graviton2 and containers",
	"tags": [],
	"description": "",
	"content": "In this Chapter, we will explore running container workloads using native multi-architecture support in\nAmazon EKS, Amazon ECR, and AWS developer tools such as AWS CodeBuild, CodeDeploy and CodePipeline.\n"
},
{
	"uri": "/en/amazonrds.html",
	"title": "Graviton2 and databases",
	"tags": [],
	"description": "",
	"content": "Graviton2 brings better cost-performance for your Amazon Relational Database Service (RDS) databases, compared to the previous M5 and R5 generation of database instance types, with the availability of AWS Graviton2 processors for RDS.\nIn this Section, we will explore running Amazon Relational Database Service (RDS) with Graviton2 instance types. We will focus on MySQL database backend, however the outlined processed are applicable to other database engines database engines (MySQL 8.0.17 and higher, MariaDB 10.4.13 and higher, and PostgreSQL 12.3 and higher) supported by RDS for Gravtion2. When selecting Graviton2 instance types for RDS and you can choose between M6g and R6g instance families.\nM6g instances are ideal for general purpose workloads. R6g instances offer 50% more memory than their M6g counterparts and are ideal for memory intensive workloads, such as Big Data analytics.\nYou can explore one of two paths for moving your exisint open source database backend to Graviton2 instance types:\nOption 1) Creating a new database from compatible databse version snapshot (recommended for production process)\nOption 2) Upgrading your older database to a version supported by RDS with Graviton2, and modyfying instance type (test/dev environment)\n"
},
{
	"uri": "/en/amazonemr.html",
	"title": "Graviton2 and big data",
	"tags": [],
	"description": "",
	"content": "In this Chapter, we will explore running Amazon EMR cluster on Graviton2 instance types. EMR natively supports Graviton2 instances, hence in this scenario we are focusing on simple Spark job execution.\n"
},
{
	"uri": "/en/amazoncontainers/amazonecs/ecscluster.html",
	"title": "AmazonECS : Deploy Graviton2 ECS cluster",
	"tags": [],
	"description": "",
	"content": "Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast container management service that makes it easy to run, stop, and manage containers on a cluster. Your containers are defined in a task definition that you use to run individual tasks or tasks within a service. Amazon ECS enables you to launch and stop your container-based applications by using simple API calls. You can also retrieve the state of your cluster from a centralized service and have access to many familiar Amazon EC2 features.\nMake sure you complete Multi-architecture application build lab before starting this one.\n Start by reviewing the contents of the AmazonECS task definition in file graviton2-labs/graviton2/cs_graviton/ecs_graviton2.py\necs_ami = ecs.EcsOptimizedAmi(generation=ec2.AmazonLinuxGeneration.AMAZON_LINUX_2, hardware_type=ecs.AmiHardwareType.ARM) asg_ecs = cluster.add_capacity(\u0026#34;G2AutoScalingGroup\u0026#34;, instance_type=ec2.InstanceType(\u0026#34;m6g.2xlarge\u0026#34;), machine_image=ecs_ami ) container = task_definition.add_container( \u0026#34;web\u0026#34;, image=ecs.ContainerImage.from_registry(\u0026#34;{{container_uri}}\u0026#34;), memory_limit_mib=512, logging=ecs.LogDrivers.firelens( options={ \u0026#34;Name\u0026#34;: \u0026#34;cloudwatch\u0026#34;, \u0026#34;log_key\u0026#34;: \u0026#34;log\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;delivery_stream\u0026#34;: \u0026#34;my-stream\u0026#34;, \u0026#34;log_group_name\u0026#34;: \u0026#34;firelens-fluent-bit\u0026#34;, \u0026#34;auto_create_group\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;log_stream_prefix\u0026#34;: \u0026#34;from-fluent-bit\u0026#34;} ) )  Deploy ECS cluster using CDK  cdk deploy GravitonID-ecs Once the cluster and ECS task are available execute following commands to verify ELB endpoint for the service.  It will take several minutes for the ELB to become healthy and start passing traffic to the pods.\n ECS_ELB=$(aws cloudformation describe-stacks --stack-name GravitonID-ecs --query \u0026#34;Stacks[0].Outputs[0].OutputValue\u0026#34; --output text) Test the cluster and verify your service using Graviton2 instance type.  for i in {1..8}; do curl -m3 $ECS_ELB ; echo; done You now have a fully wowking ECS cluster using Graviton2 instace type that is ready to use.\n"
},
{
	"uri": "/en/amazoncontainers/cleanup.html",
	"title": "Graviton2 and Containers: Cleanup",
	"tags": [],
	"description": "",
	"content": "Execute following commands to clean up the environment.\ncd ~/environment/graviton2-labs/ scripts/cs_cleanup.sh "
},
{
	"uri": "/en/categories.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/en/tags.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]